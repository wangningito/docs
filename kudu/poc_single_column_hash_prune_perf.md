Idea: reduce the in-list predicate values to be pushed down to each tablet with the help of hash bucket calculation.

Detail of idea and background is in: https://gerrit.cloudera.org/#/c/16674/

Result is on the last page. This test only tested the bench of  java port clients. 
However I used impala to check whether it was pruned or not. It should have similar performance as java port have.

Test settings: 
server and client and on same node
server:  
RAM:  24g for kudu tserver
CPU: 8 threads AMD EPYC2: 2.9GHz-3.4GHz
OS: CentOS Linux release 7.6.1810 (Core) 
DISK: one ssd for blocks and wal dir
Flag settings: 
--block_cache_capacity_mb=1 (I closed cache block in code, in here is double check)
--unlock_experimental_flags=true 
client : 
java client, openjdk version "1.8.0_262"
version: 1.13.0 from maven

Table settings: 30 columns x 1 00 000 000 rows 
Table structure: 
TABLE profile_table (  
    id INT64 NOT NULL,
    ffset INT64 NOT NULL,
    first_id STRING NULLABLE,
    second_id STRING NULLABLE,
    hh_device_id_list STRING NULLABLE,
    …...
    …… (types are INT64 and STRING, half-half)
    .…..
    hh_utm_source STRING NULLABLE,
    hh_utm_term STRING NULLABLE,
    PRIMARY KEY (id)
)
HASH (id) PARTITIONS 6
REPLICAS 1




Data generating:
kudu perf loadgen kudu-release.sa -table_name=profile_table -use_random=true --num_rows_per_thread=25000000 -num_threads=4
(Wait until all tablets are well compacted, height of all tablets are 1 as I see in tablet page)


Code fraction 
-----------------------------------------
List<Integer> projectIndex = new ArrayList<Integer>() {{
   add(1); add(2); add(3); add(4); add(5);
}};

for (int i = 0; i < 1000000; i+= onceCount, cnt += 1) {
   KuduPredicate predicate = KuduPredicate.newInListPredicate(id, allIDs.subList(i, i+onceCount));

   long now = System.currentTimeMillis();
   List<KuduScanToken> tokens = kuduClient.newScanTokenBuilder(kuduTable).
           setProjectedColumnIndexes(projectIndex).cacheBlocks(false).addPredicate(predicate).build();

   tokens.parallelStream().forEach(kuduScanToken -> {
       try {
           KuduScanner kuduScanner = kuduScanToken.intoScanner(kuduClient);
           while (kuduScanner.hasMoreRows()) {
               kuduScanner.nextRows();
           }
       } catch (Exception e) {
           e.printStackTrace(); 
       }
   });

   long end = System.currentTimeMillis();

-----------------------------------------
allIds is generated by 
impala-shell -q ”select id from profile_table order by rand(1) limit 1000000”
and load into memory in code before this fraction.

Also, projected only 5 columns to minimize the effect of the network.














Results (projected 5 columns)
token predicate (concurrency 1).
pruned? / batch size
50
100
1000
10000
no prune
1173ms/batch
1355ms/batch
2133ms/batch
4964ms/batch
pruned
771ms/batch
1008ms/batch
1800ms/batch
4700ms/batch


token predicate (concurrency 2)
pruned? / batch size
50
100
1000
10000
no prune
660ms/batch
730ms/batch
1181ms/batch
2900ms/batch
pruned
400ms/batch
520ms/batch
1035ms/batch
2775ms/batch


token predicate (concurrency 4)
pruned? / batch size
50
100
1000
10000
no prune
404ms/batch
461ms/batch
810ms/batch
2200ms/batch
pruned
270ms/batch
347ms/batch
709ms/batch
2015ms/batch


token predicate (no limit, same as 6)
pruned? / batch size
50
100
1000
10000
no prune
250ms/batch
284ms/batch
480ms/batch
1700ms/batch
pruned
173ms/batch
218ms/batch
414ms/batch
1329ms/batch


Result in this below sheet can be explain as: 
It takes 1173ms to query 50 values with in-list predicates, the predicate pushed to the tablet is executed one by one(concurrency = 1). 

token predicate (concurrency 1).
pruned? / batch size
50
no prune
1173ms/batch


Suppose P to be partitions count of a table. (More ideas are written in gerrit commit msg).
Theoretically, the promotion of performance is roughly log2(P). (refer to gerrit msg), in this case, it should be 2.44. However, in case of batch size 50, it only varies from 1.44-1.52. It’s even less when batch size increases.
The promotion of performance seems to lose when in-list predicate value count begins to raise. 
As I guess, that’s caused by network burden, in case of a 10000 size batch, 6 partitions, it should call HandleNewScanRequests 6 times thus it has to upload all predicate values 6 times. 

Another very slight benefit, it can rewrite the upper bound and lower bound in case bound key is pruned, thus it can reduce the blocks to scan, however very very very slightly: 

Before:

After:


